# To modify an existing job, simply update the jobs_type: dictionary
# commit, push
# Then click 'deploy' on the jenkins job related to this repository
# available on the jenkins homepage.

project: 'ClusterHQ/flocker'
git_url: 'https://github.com/ClusterHQ/flocker.git'

# common_cli contains YAML aliases for common cli operations
# used during the build process
common_cli:
  hashbang: &hashbang |
    #!/bin/bash
    set -x
    set -e

  cleanup: &cleanup |
    export PATH=/usr/local/bin:$PATH
    # clean up the stuff from previous runs
    # due to the length of the jobname workspace, we are hitting limits in
    # our sheebang path name in pip.
    # So we will place the virtualenv in /tmp/v instead
    sudo rm -rf /tmp/v
    sudo rm -f results.xml
    sudo rm -f trial.log
    sudo rm -rf _trial_temp/

  setup_venv: &setup_venv |
    # setup the new venv
    virtualenv -p python2.7 --clear /tmp/v
    . /tmp/v/bin/activate

  setup_pip_accel: &setup_pip_accel |
    # use S3 pip binary caching
    pip install "pip-accel[s3]"
    export PIP_ACCEL_S3_BUCKET=clusterhq-pip-accel
    # export the AWS secrets, so that pip-accel can connect to S3
    . /etc/boto.sh

  setup_pip_accel_centos7_vars: &setup_pip_accel_centos7_vars |
    export PIP_ACCEL_S3_PREFIX=centos-7

  setup_pip_accel_ubuntu_trusty_vars: &setup_pip_accel_ubuntu_trusty_vars |
    export PIP_ACCEL_S3_PREFIX=ubuntu-trusty-64

  setup_flocker_modules: &setup_flocker_modules |
    pip-accel install . --retries 15 --timeout 30 --disable-pip-version-check
    pip-accel install Flocker[doc,dev,release] python-subunit junitxml --retries 15 --timeout 30 --disable-pip-version-check

  setup_aws_env_vars: &setup_aws_env_vars |
    # set vars and run tests
    export FLOCKER_FUNCTIONAL_TEST_CLOUD_CONFIG_FILE=/not-found/acceptance.yml
    export FLOCKER_FUNCTIONAL_TEST_AWS_AVAILABILITY_ZONE=us-west-2c
    export FLOCKER_FUNCTIONAL_TEST_CLOUD_PROVIDER=aws
    # need to unset the AWS secrets used by pip-accel
    unset AWS_ACCESS_KEY_ID
    unset AWS_SECRET_ACCESS_KEY

  setup_coverage: &setup_coverage |
    pip-accel install coverage==3.7.1 http://data.hybridcluster.net/python/coverage_reporter-0.01_hl0-py27-none-any.whl

  run_coverage: &run_coverage |
    coverage xml --include=flocker*

  convert_results_to_junit: &convert_results_to_junit |
    cat trial.log | subunit-1to2 | subunit2junitxml --no-passthrough --output-to=results.xml

  run_sphinx: &run_sphinx |
    python setup.py --version
    cd docs
    set +e #don't abort at the first failure
    let status=0
    sphinx-build -d _build/doctree -b spelling . _build/spelling
    let status=status+\$?
    sphinx-build -d _build/doctree -b linkcheck . _build/linkcheck
    let status=status+\$?
    sphinx-build -d _build/doctree -b html . _build/html
    exit \$status
    # upload html
    #link-release-documentation
    #upload-release-documentation
    cd -

  flocker_artifacts: &flocker_artifacts
    - results.xml
    - _trial_temp/test.log
    - coverage.xml

  run_trial_with_coverage: &run_trial_with_coverage |
    # Consume the MODULE parameter set in the job configuration
    coverage run /tmp/v/bin/trial --reporter=subunit \$MODULE 2>&1 | tee trial.log

  setup_authentication: &setup_authentication |
    # acceptance tests rely on this file existing
    touch \$HOME/.ssh/known_hosts
    # remove existing keys
    rm -f \$HOME/.ssh/id_rsa*
    cp /tmp/id_rsa \$HOME/.ssh/id_rsa
    chmod -R 0700 \$HOME/.ssh
    ssh-keygen -N '' -f \$HOME/.ssh/id_rsa_flocker
    eval `ssh-agent -s`
    ssh-add \$HOME/.ssh/id_rsa

  run_acceptance_tests: &run_acceptance_tests
    '/tmp/v/bin/python admin/run-acceptance-tests --distribution centos-7 --provider aws --dataset-backend aws --branch \$TRIGGERED_BRANCH --build-server http://build.clusterhq.com --config-file /tmp/acceptance.yaml'

  check_version: &check_version |
    export FLOCKER_VERSION=\$(/tmp/v/bin/python setup.py --version)

  build_sdist: &build_sdist
    '/tmp/v/bin/pyhon setup.py sdist'

  build_package: &build_package
      '/tmp/v/bin/python admin/build-package --destination-path repo --distribution centos-7 /flocker/dist/Flocker-\${FLOCKER_VERSION}.tar.gz'

# the job definitions, they consume the aliases defined in the common_cli block
#
# flocker.node.functional is hanging, so we don't run it
job_type:
  run_trial:
  # http://build.clusterhq.com/builders/flocker-centos-7
    run_trial_on_AWS_CentOS_7:
      on_nodes_with_labels: 'aws-centos-7-T2Micro'
      with_modules:
        - flocker.acceptance
        - flocker.ca.functional
        - flocker.ca.test
        - flocker.cli
        - flocker.common
        - flocker.control
        - flocker.node.agents
        - flocker.node.test
        # TODO:
        # one of the functional tests is hanging, so we split the functional
        # tests and comment out the subset that is hanging
        # - flocker.node.functional
        # - flocker.node.functional.test_docker
        - flocker.node.functional.test_script
        - flocker.node.functional.test_deploy
        - flocker.provision
        - flocker.restapi
        - flocker.route
        - flocker.test
        - flocker.testtools
        - flocker.volume
      with_steps:
        - { type: 'shell',
            cli: [ *hashbang, *setup_pip_accel_centos7_vars,
                   *cleanup, *setup_venv, *setup_pip_accel, *setup_flocker_modules, *setup_coverage, *setup_aws_env_vars,
                   *run_trial_with_coverage, *run_coverage,
                   *convert_results_to_junit ]
          }
      archive_artifacts: *flocker_artifacts
      coverage_report: true

# http://build.clusterhq.com/builders/flocker-admin
# http://build.clusterhq.com/builders/flocker-ubuntu-14.04
    run_trial_on_AWS_Ubuntu_Trusty:
      on_nodes_with_labels: 'aws-ubuntu-trusty-T1Micro'
      with_modules:
        - admin
        - flocker.acceptance
        - flocker.ca.functional
        - flocker.ca.test
        - flocker.cli
        - flocker.common
        - flocker.control
        - flocker.node.agents
        - flocker.node.test
        # TODO:
        # one of the functional tests is hanging, so we split the functional
        # tests and comment out the subset that is hanging
        # - flocker.node.functional
        # - flocker.node.functional.test_docker
        - flocker.node.functional.test_script
        - flocker.node.functional.test_deploy
        - flocker.provision
        - flocker.restapi
        - flocker.route
        - flocker.test
        - flocker.testtools
        - flocker.volume
      with_steps:
        - { type: 'shell',
            cli: [ *hashbang, *setup_pip_accel_ubuntu_trusty_vars,
                   *cleanup, *setup_venv, *setup_pip_accel, *setup_flocker_modules, *setup_coverage, *setup_aws_env_vars,
                   *run_trial_with_coverage, *run_coverage,
                   *convert_results_to_junit ]
          }
      archive_artifacts: *flocker_artifacts
      coverage_report: true

  # http://build.clusterhq.com/builders/flocker-docs
  run_sphinx:
    run_sphinx:
      on_nodes_with_labels: 'aws-centos-7-T2Micro'
      with_steps:
        - { type: 'shell',
            cli: [ *hashbang, *setup_pip_accel_centos7_vars,
                   *cleanup, *setup_venv, *setup_pip_accel, *setup_flocker_modules, *setup_aws_env_vars,
                   *run_sphinx ]
          }

  # http://build.clusterhq.com/builders/flocker%2Facceptance%2Faws%2Fcentos-7%2Faws
  run_acceptance:
    run_acceptance_on_AWS_CentOS_7:
      on_nodes_with_labels: 'aws-centos-7-M3Medium'
      with_steps:
        - { type: 'shell',
            cli: [ *hashbang, *setup_pip_accel_centos7_vars,
               *cleanup, *setup_venv, *setup_pip_accel, *setup_flocker_modules, *setup_aws_env_vars,
               *setup_authentication, *run_acceptance_tests ]
          }

  omnibus:
    run_omnibus_for_CentOS_7:
      on_nodes_with_labels: 'aws-ubuntu-trusty'
      with_steps:
        - { type: 'shell',
            cli: [ *hashbang, *setup_pip_accel_ubuntu_trusty_vars,
               *cleanup, *setup_venv, *setup_pip_accel, *setup_flocker_modules, *setup_aws_env_vars,
               *check_version, *build_sdist, *build_package ]
          }
